{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dandyshu/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/api.py:77: RuntimeWarning: '<' not supported between instances of 'str' and 'int', sort order is undefined for incomparable objects\n",
      "  result = result.union(other)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status updated for Batch 99982/input-valve\n",
      "Status updated for Batch 99982/output-valve\n",
      "Status updated for Batch 44772/input-valve\n",
      "Status updated for Batch 44772/output-valve\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-38bed02f48fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-80-38bed02f48fb>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;31m#run time interval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTIME_INTERVAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "API_URL = \"http://minim-phxap-h8qgv5ltdcsr-1721285542.us-east-1.elb.amazonaws.com/bioreactor/0\"\n",
    "DETAILS_API_URL = \"http://minim-phxap-h8qgv5ltdcsr-1721285542.us-east-1.elb.amazonaws.com/bioreactor/{}\"\n",
    "\n",
    "\n",
    "TIME_INTERVAL = 10  # Time interval in seconds\n",
    "\n",
    "def fetch_id():\n",
    "    response = requests.get(API_URL)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Failed to fetch data. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def fetch_details(batch_id):\n",
    "    details_url = DETAILS_API_URL.format(batch_id)\n",
    "    response = requests.get(details_url)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Failed to fetch details for ID {batch_id}. Status code: {response.status_code}\")\n",
    "        return None\n",
    "    \n",
    "def create_csv_file(file_name, header):\n",
    "    if not os.path.exists(file_name):\n",
    "        with open(file_name, 'a', newline='') as csv_file:\n",
    "            csv_writer = csv.writer(csv_file)\n",
    "            csv_writer.writerow(header)\n",
    "            \n",
    "def update_status(id_update_url,status):\n",
    "    status_data = {\"status\": status}\n",
    "    status_update_url = DETAILS_API_URL.format(id_update_url)\n",
    "    response = requests.put(status_update_url, json=status_data)\n",
    "    if response.status_code == 200:\n",
    "        print(f\"Status updated for Batch {id_update_url}\")\n",
    "    else:\n",
    "        print(f\"Failed to update status for Batch {id_update_url}. Status code: {response.status_code}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    #create raw file header\n",
    "    raw_file_header =['timestamp','batch_id','fill_percent','temperature','ph_value','pressure','input_state','output_state']\n",
    "    #target aggregated file header\n",
    "    agg_file_header =['batch_id','success','fill_level','temperature_range','ph_range','presure_range',\n",
    "                      'total_time','met_fill_level','met_temperature_level','met_presure_level']\n",
    "    #create empty df for aggregation with same header as raw data \n",
    "    empty_df = pd.DataFrame(columns = raw_file_header)\n",
    "    \n",
    "    while True:\n",
    "        data = fetch_id()\n",
    "        if not data:\n",
    "            print(\"This vessel is emptied and the batch process is done.\")\n",
    "            break\n",
    "        batch_id = data['id']\n",
    "        # Fetch details \n",
    "        details = fetch_details(batch_id)\n",
    "        #Fetch input and output state\n",
    "        input_id = batch_id + '/input-valve'\n",
    "        input_info = fetch_details(input_id)\n",
    "        output_id = batch_id + '/output-valve'\n",
    "        output_info = fetch_details(output_id)\n",
    "        \n",
    "        #create raw data file, since streaming data can be accumulated to extremely large so I split the file by date\n",
    "        #we don't have to split the file if the dataset is small or split by bigger date range\n",
    "        dataload_time = datetime.now().date()\n",
    "        mes_raw_file = f\"mes_raw_{dataload_time}.csv\"\n",
    "        if not os.path.exists(mes_raw_file):\n",
    "            create_csv_file(mes_raw_file,raw_file_header)\n",
    "        \n",
    "        #load streaming data to csv file,data will be loaded to a new file each day\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        target_list = [timestamp, batch_id, details['fill_percent'],details['temperature'],\n",
    "                                 details['pH'],details['pressure'],input_info['state'],output_info['state']]\n",
    "        with open(mes_raw_file, 'a', newline='') as csv_file:\n",
    "            csv_writer = csv.writer(csv_file)\n",
    "            csv_writer.writerow(target_list)\n",
    "        \n",
    "        #calculate the aggregations with pandas dataframe\n",
    "        df = empty_df.append(target_list,ignore_index=True)\n",
    "        \n",
    "        if details['fill_percent'] >=68 and details['fill_percent']<=72 and details['temperature']>=79 and details['temperature']<=81 and details['pressure']<200:\n",
    "            success = 'Y'\n",
    "            fill_level = details['fill_percent']\n",
    "            temperature_range = df[df['batch_id']==batch_id].groupby('batch_id')['temperature'].max() - df[df['batch_id']==batch_id].groupby('batch_id')['temperature'].min()\n",
    "            ph_range = df[df['batch_id']==batch_id].groupby('batch_id')['ph_value'].max() - df[df['batch_id']==batch_id].groupby('batch_id')['ph_value'].min()\n",
    "            presure_range = df[df['batch_id']==batch_id].groupby('batch_id')['pressure'].max() - df[df['batch_id']==batch_id].groupby('batch_id')['pressure'].min()\n",
    "            total_time = df[df['batch_id']==batch_id].groupby('batch_id')['timestamp'].max() - df[df['batch_id']==batch_id].groupby('batch_id')['timestamp'].min()\n",
    "            if details['fill_percent'] >=68 and details['fill_percent']<=72:\n",
    "                met_fill_level = 'Y'\n",
    "            else:\n",
    "                met_fill_level = 'N'\n",
    "            if details['temperature']>=79 and details['temperature']<=81:\n",
    "                met_temperature_level = 'Y'\n",
    "            else:\n",
    "                met_temperature_level = 'N'\n",
    "            met_presure_level = 'Y'\n",
    "            \n",
    "            #write to target file\n",
    "            agg_list = [batch_id,success,fill_level,temperature_range,ph_range,presure_range,\n",
    "                        total_time,met_fill_level,met_temperature_level,met_presure_level]\n",
    "            mes_agg_file = f\"mes_agg_{dataload_time}.csv\"\n",
    "            #create aggregation file each day\n",
    "            if not os.path.exists(mes_agg_file):\n",
    "                create_csv_file(mes_agg_file,agg_file_header)\n",
    "            with open(mes_agg_file, 'a', newline='') as agg_csv_file:\n",
    "                csv_writer = csv.writer(agg_csv_file)\n",
    "                csv_writer.writerow(agg_list)\n",
    "            \n",
    "            \n",
    "            #delete the compeleted batch info,so that the computing dataframe won't go too big\n",
    "            df = df[df['batch_id'] != batch_id]\n",
    "            \n",
    "            #one batch is over set input state to closed and output to open\n",
    "            inport_status = 'closed'\n",
    "            update_status(input_id,inport_status)\n",
    "            outport_status = 'open'\n",
    "            update_status(output_id,inport_status)\n",
    "            \n",
    "        #abort batch  \n",
    "        elif details['pressure']>=200:\n",
    "            success = 'N'\n",
    "            fill_level = details['fill_percent']\n",
    "            temperature_range = df[df['batch_id']==batch_id].groupby('batch_id')['temperature'].max() - df[df['batch_id']==batch_id].groupby('batch_id')['temperature'].min()\n",
    "            ph_range = df[df['batch_id']==batch_id].groupby('batch_id')['ph_value'].max() - df[df['batch_id']==batch_id].groupby('batch_id')['ph_value'].min()\n",
    "            presure_range = df[df['batch_id']==batch_id].groupby('batch_id')['pressure'].max() - df[df['batch_id']==batch_id].groupby('batch_id')['pressure'].min()\n",
    "            total_time = df[df['batch_id']==batch_id].groupby('batch_id')['timestamp'].max() - df[df['batch_id']==batch_id].groupby('batch_id')['timestamp'].min()\n",
    "            if details['fill_percent'] >=68 and details['fill_percent']<=72:\n",
    "                met_fill_level = 'Y'\n",
    "            else:\n",
    "                met_fill_level = 'N'\n",
    "            if details['temperature']>=79 and details['temperature']<=81:\n",
    "                met_temperature_level = 'Y'\n",
    "            else:\n",
    "                met_temperature_level = 'N'\n",
    "            met_presure_level = 'N'\n",
    "            \n",
    "            #write to target file\n",
    "            agg_list = [batch_id,success,fill_level,temperature_range,ph_range,presure_range,\n",
    "                        total_time,met_fill_level,met_temperature_level,met_presure_level]\n",
    "            mes_agg_file = f\"mes_agg_{dataload_time}.csv\"\n",
    "            #create aggregation file each day\n",
    "            if not os.path.exists(mes_agg_file):\n",
    "                create_csv_file(mes_agg_file,agg_file_header)\n",
    "            with open(mes_agg_file, 'a', newline='') as agg_csv_file:\n",
    "                csv_writer = csv.writer(agg_csv_file)\n",
    "                csv_writer.writerow(agg_list)\n",
    "            \n",
    "            #delete the compeleted batch info,so that the computing dataframe won't go too big\n",
    "            df = df[df['batch_id'] != batch_id]\n",
    "            \n",
    "            #one batch is aborted set input state to closed and output to open\n",
    "            inport_status = 'closed'\n",
    "            update_status(input_id,inport_status)\n",
    "            outport_status = 'open'\n",
    "            update_status(output_id,inport_status)\n",
    "        #batch is running\n",
    "        else:\n",
    "            #one batch is over set input state to closed and output to open\n",
    "            inport_status = 'open'\n",
    "            update_status(input_id,inport_status)\n",
    "            outport_status = 'closed'\n",
    "            update_status(output_id,inport_status)\n",
    "        \n",
    "        \n",
    "        #run time interval\n",
    "        time.sleep(TIME_INTERVAL)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "etl process:\n",
    "1.get all streaming data in json format and then load to csv file\n",
    "2.store the temp data into dataframe and do the calculations\n",
    "3.store all the aggregated data to csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>closed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    state\n",
       "0  closed"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "id ='http://minim-phxap-h8qgv5ltdcsr-1721285542.us-east-1.elb.amazonaws.com/bioreactor/0'\n",
    "link ='http://minim-phxap-h8qgv5ltdcsr-1721285542.us-east-1.elb.amazonaws.com/bioreactor/'\n",
    "output = 'http://minim-phxap-h8qgv5ltdcsr-1721285542.us-east-1.elb.amazonaws.com/bioreactor/43381/output-valve'\n",
    "input = 'http://minim-phxap-h8qgv5ltdcsr-1721285542.us-east-1.elb.amazonaws.com/bioreactor/43381/input-valve'\n",
    "\n",
    "inpu = requests.get(id)\n",
    "j = inpu.json()\n",
    "#dict1 = json.loads(j)\n",
    "df2 = pd.DataFrame(j,index=[0]) \n",
    "df2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
